{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Homework\n",
    "Use this notebook to answer the questions.\n",
    "It can be in the same project as the previous homework.  \n",
    "When you are ready, **upload** to your github repo, and send me the link (just zip a txt file with the repo's address, and upload it as the homework).  \n",
    "If your repo is private, invite me: balazs.balogh@cubixedu.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the SparkSession, create it then load the taxi data (yellow_tripdata_2024-08.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-08-01 00:21:00|  2024-08-01 00:36:13|              1|          7.4|         1|                 N|         138|          80|           1|       28.9| 7.75|    0.5|      7.65|         0.0|                  1.0|        45.8|                 0.0|       1.75|\n",
      "|       2| 2024-08-01 00:20:01|  2024-08-01 00:41:47|              1|         9.91|         1|                 N|         138|         239|           1|       40.8|  6.0|    0.5|     11.55|        6.94|                  1.0|       71.04|                 2.5|       1.75|\n",
      "|       1| 2024-08-01 00:17:52|  2024-08-01 00:41:45|              0|         13.4|         1|                 N|         138|          88|           1|       52.0|10.25|    0.5|      15.0|         0.0|                  1.0|       78.75|                 2.5|       1.75|\n",
      "|       1| 2024-08-01 00:49:08|  2024-08-01 00:55:56|              0|          3.9|         1|                 N|         209|         137|           3|       17.0|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.0|                 2.5|        0.0|\n",
      "|       1| 2024-08-01 00:38:52|  2024-08-01 00:42:34|              1|          0.4|         1|                 N|         148|         144|           2|        5.1|  3.5|    0.5|       0.0|         0.0|                  1.0|        10.1|                 2.5|        0.0|\n",
      "|       1| 2024-08-01 00:57:59|  2024-08-01 01:03:14|              1|          0.4|         1|                 N|         148|         144|           1|        5.8|  3.5|    0.5|      2.15|         0.0|                  1.0|       12.95|                 2.5|        0.0|\n",
      "|       2| 2024-08-01 00:15:46|  2024-08-01 00:29:45|              2|         3.21|         1|                 N|         211|         233|           1|       16.3|  1.0|    0.5|      2.13|         0.0|                  1.0|       23.43|                 2.5|        0.0|\n",
      "|       2| 2024-08-01 00:32:17|  2024-08-01 00:45:53|              1|          3.8|         1|                 N|         170|         239|           1|       17.7|  1.0|    0.5|      4.54|         0.0|                  1.0|       27.24|                 2.5|        0.0|\n",
      "|       2| 2024-08-01 00:48:16|  2024-08-01 01:09:23|              1|         5.54|         1|                 N|         239|         148|           1|       27.5|  1.0|    0.5|       6.5|         0.0|                  1.0|        39.0|                 2.5|        0.0|\n",
      "|       2| 2024-07-31 23:30:15|  2024-07-31 23:40:24|              1|         1.56|         1|                 N|         164|         229|           1|       11.4|  1.0|    0.5|       3.0|         0.0|                  1.0|        19.4|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    DateType,\n",
    "    TimestampType\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"SQL\")        # type: ignore\n",
    "    .master(\"local[*]\")    # Ensures it runs in local mode using all CPU cores (6 cores and 12 threads in my case).\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "taxi_data_2024_08 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"parquet\")\n",
    "    .load(\"C:\\\\Users\\\\Lerry\\\\Desktop\\\\cubix_data_engineer_homework\\\\scr\\data\\\\yellow_tripdata_2024-08.parquet\")\n",
    ")\n",
    "\n",
    "taxi_data_2024_08.createOrReplaceTempView(\"taxi_data_2024_08\")\n",
    "\n",
    "taxi_data_2024_08.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the total fare amount for all trips?  \n",
    "Please round the answer to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_fare_amount|\n",
      "+-----------------+\n",
      "|    5.875099806E7|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "   SELECT\n",
    "        ROUND(SUM(fare_amount), 2) AS total_fare_amount\n",
    "    FROM \n",
    "        taxi_data_2024_08\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show the maximum fare amount, minimum fare amount, and average fare amount for each payment type. Order by payment type.\n",
    "Round where you need to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------------+---------------+\n",
      "|payment_type|max_fare_amount|min_fare_amount|avg_fare_amount|\n",
      "+------------+---------------+---------------+---------------+\n",
      "|           1|          650.0|         -108.7|           20.6|\n",
      "|           3|          999.0|         -999.0|           6.26|\n",
      "|           2|         1386.2|        -1174.1|          19.09|\n",
      "|           4|          900.0|         -900.0|           1.37|\n",
      "|           0|         394.76|          -72.2|          19.62|\n",
      "+------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "    SELECT\n",
    "        payment_type,\n",
    "        ROUND(MAX(fare_amount), 2) AS max_fare_amount,\n",
    "        ROUND(MIN(fare_amount), 2) AS min_fare_amount,\n",
    "        ROUND(AVG(fare_amount), 2) AS avg_fare_amount\n",
    "    FROM\n",
    "        taxi_data_2024_08\n",
    "    GROUP BY\n",
    "        payment_type\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. For trips with a fare amount greater than 20, what is the total tip amount for each day (based on the tpep_pickup_datetime)?\n",
    "Round the tip to two decimal places, and order the results from highest total tip amount.  \n",
    "Hint: Check DATE() function, to convert tpep_pickup_datetime to date, to get only the YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|      days|total_tip_amount|\n",
      "+----------+----------------+\n",
      "|2024-08-07|       200170.81|\n",
      "|2024-08-01|       197694.16|\n",
      "|2024-08-29|       192837.64|\n",
      "|2024-08-08|       192807.33|\n",
      "|2024-08-19|       184872.55|\n",
      "|2024-08-22|       182233.24|\n",
      "|2024-08-30|       180368.43|\n",
      "|2024-08-28|       180220.74|\n",
      "|2024-08-05|       176271.55|\n",
      "|2024-08-15|        176223.0|\n",
      "|2024-08-06|       175451.67|\n",
      "|2024-08-11|       174769.89|\n",
      "|2024-08-20|       173310.48|\n",
      "|2024-08-25|       171774.94|\n",
      "|2024-08-04|       169589.92|\n",
      "|2024-08-23|        168495.1|\n",
      "|2024-08-02|        167908.5|\n",
      "|2024-08-14|        166266.4|\n",
      "|2024-08-21|       165647.98|\n",
      "|2024-08-27|        164666.8|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "    SELECT\n",
    "    DATE(tpep_pickup_datetime) AS days,\n",
    "    ROUND(SUM(tip_amount), 2) AS total_tip_amount\n",
    "    \n",
    "    FROM\n",
    "        taxi_data_2024_08\n",
    "          \n",
    "    WHERE\n",
    "          fare_amount > 20\n",
    "    GROUP BY\n",
    "        days\n",
    "    ORDER BY total_tip_amount DESC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. For each trip, show the fare amount along with a column that indicates if the trip was \"expensive\" (greater than 30) or \"cheap\" (less than or equal to 30).\n",
    "Hint: Use CASE WHEN for deciding on expensive, or cheap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|fare_amount|fare_category|\n",
      "+-----------+-------------+\n",
      "|       28.9|        cheap|\n",
      "|       40.8|    expensive|\n",
      "|       52.0|    expensive|\n",
      "|       17.0|        cheap|\n",
      "|        5.1|        cheap|\n",
      "|        5.8|        cheap|\n",
      "|       16.3|        cheap|\n",
      "|       17.7|        cheap|\n",
      "|       27.5|        cheap|\n",
      "|       11.4|        cheap|\n",
      "|        9.3|        cheap|\n",
      "|        5.1|        cheap|\n",
      "|       16.3|        cheap|\n",
      "|       45.0|    expensive|\n",
      "|        8.6|        cheap|\n",
      "|        7.2|        cheap|\n",
      "|       13.5|        cheap|\n",
      "|       14.9|        cheap|\n",
      "|       10.7|        cheap|\n",
      "|        8.6|        cheap|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    \n",
    "    SELECT\n",
    "        fare_amount,\n",
    "        CASE\n",
    "            WHEN fare_amount > 30 THEN 'expensive'\n",
    "            ELSE 'cheap'\n",
    "        END AS fare_category\n",
    "    FROM \n",
    "        taxi_data_2024_08\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Find the first trip (based on tpep_pickup_datetime) for each VendorID and display the fare amount.\n",
    "Hint: You can use CTE with ROW_NUMBER()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|VendorID|fare_amount|\n",
      "+--------+-----------+\n",
      "|       1|       70.0|\n",
      "|       2|       10.7|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "    WITH first_trip AS (\n",
    "        SELECT\n",
    "            tpep_pickup_datetime,\n",
    "            fare_amount,\n",
    "            VendorID,\n",
    "            ROW_NUMBER() OVER (PARTITION BY VendorID ORDER BY tpep_pickup_datetime) AS row_num\n",
    "        FROM\n",
    "            taxi_data_2024_08\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        VendorID,\n",
    "        fare_amount\n",
    "    FROM\n",
    "        first_trip\n",
    "    WHERE row_num = 1\n",
    "              \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Calculate the average trip distance for each VendorID, and assign a label of 'Above Average' or 'Below Average' for each trip based on the distance relative to the VendorIDâ€™s average trip distance.\n",
    "Hint: CTE joined back to the main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+\n",
      "|VendorID|trip_distance|distance_label|\n",
      "+--------+-------------+--------------+\n",
      "|       1|          7.4| Above Average|\n",
      "|       2|         9.91| Above Average|\n",
      "|       1|         13.4| Above Average|\n",
      "|       1|          3.9| Below Average|\n",
      "|       1|          0.4| Below Average|\n",
      "|       1|          0.4| Below Average|\n",
      "|       2|         3.21| Below Average|\n",
      "|       2|          3.8| Below Average|\n",
      "|       2|         5.54| Above Average|\n",
      "|       2|         1.56| Below Average|\n",
      "|       2|         1.32| Below Average|\n",
      "|       2|         0.45| Below Average|\n",
      "|       2|         2.65| Below Average|\n",
      "|       1|         11.7| Above Average|\n",
      "|       2|         1.54| Below Average|\n",
      "|       1|          0.4| Below Average|\n",
      "|       2|         2.19| Below Average|\n",
      "|       2|         2.58| Below Average|\n",
      "|       1|          1.4| Below Average|\n",
      "|       1|          1.1| Below Average|\n",
      "+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "    WITH vendor_avg_trip_distance AS (\n",
    "        SELECT\n",
    "            VendorID,\n",
    "            AVG(trip_distance) AS avg_trip_distance\n",
    "        FROM\n",
    "            taxi_data_2024_08  \n",
    "        GROUP BY\n",
    "            VendorID\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        t.VendorID,\n",
    "        t.trip_distance,\n",
    "        CASE\n",
    "            WHEN t.trip_distance > v.avg_trip_distance THEN 'Above Average'\n",
    "            ELSE 'Below Average'\n",
    "        END AS distance_label       \n",
    "    FROM  taxi_data_2024_08 t    \n",
    "    JOIN \n",
    "        vendor_avg_trip_distance v\n",
    "    ON \n",
    "        t.VendorID = v.VendorID\n",
    "\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
